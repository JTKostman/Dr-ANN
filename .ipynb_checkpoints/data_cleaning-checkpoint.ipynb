{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import scipy as sp;\n",
    "import re;\n",
    "import pickle;\n",
    "import os;\n",
    "import subprocess\n",
    "\n",
    "import sklearn;\n",
    "from sklearn.utils import shuffle;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer;\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def notify_slack(text):\n",
    "    subprocess.Popen('''curl -X POST --data-urlencode \"payload={'channel' : '#random', 'username': 'webhookbot', 'text':'''+ '\\'' + text + '\\'' + '''}\" https://hooks.slack.com/services/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''', shell=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will read in data to be vectorized for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "notify_slack(\"Starting data load\")\n",
    "\n",
    "def load_data():\n",
    "    diagnosis = pd.read_csv('/home/ubuntu/workspace/data/DIAGNOSES_ICD.csv');\n",
    "    notes = pd.read_csv('/home/ubuntu/workspace/data/NOTEEVENTS.csv');\n",
    "    \n",
    "    return diagnosis, notes;\n",
    "\n",
    "diagnosis, notes = load_data();\n",
    "\n",
    "diagnosis_group = diagnosis.groupby('HADM_ID').apply(lambda x: list(x['ICD9_CODE']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a custom stop words method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stop_words = str();\n",
    "    with open('nltk', 'r') as f:\n",
    "        for line in f:\n",
    "            stop_words = stop_words + '\\\\b' + line.strip() + '\\\\b' + '|';\n",
    "\n",
    "    stop_words = stop_words[0:-1];\n",
    "\n",
    "    return stop_words;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that takes in a textual clinical note and run preprocessing steps on it to remove dates, lower case all the words, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(notes_df):\n",
    "    stop_words = get_stopwords();\n",
    "    \n",
    "    # Need to remove single charcater items\n",
    "    # Need to remove the leading 0 from a digit (i.e. 07 = 7) -OR- replace with \"DIGIT\"\n",
    "    # replace dates with \"DATE\" or something \n",
    "    \n",
    "    \n",
    "    #notes_filtered = notes_df['TEXT'].apply(lambda row: re.sub(\"[^a-zA-Z0-9]\", \" \", row.lower()));\n",
    "    notes_filtered = notes_df['TEXT'].apply(lambda row: re.sub(\"21[0-9]{2}.[0-1]?[0-9]{1}.[0-3]?[0-9]{1}.+[0-2]{1}[0-9]{1}:[0-5]{1}[0-9]{1}.+[\\bAM\\b|\\bPM\\b]\", \" \", row));\n",
    "    notes_filtered = notes_filtered.apply(lambda row: re.sub(\"[^a-zA-Z0-9\\.]\", \" \", row.lower()));\n",
    "    #notes_filtered = notes_filtered.apply(lambda row: re.sub(\"\\W\\d+\\.?\\d*\", \"DIGIT\", row));\n",
    "    #notes_filtered = notes_filtered.apply(lambda row: re.sub(\"\\s[a-zA-Z]\\s\", \" \", row));                                        \n",
    "\n",
    "    notes_nostops = notes_filtered.apply(lambda row: re.sub(stop_words, ' ', row));\n",
    "    \n",
    "    notes_final = notes_nostops.apply(lambda row: \" \".join(row.split()));\n",
    "    \n",
    "    notes_df = notes_df.drop('TEXT', axis=1);\n",
    "    notes_df = notes_df.assign(TEXT = notes_final.values)\n",
    "    \n",
    "    return notes_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#notes_filtered = clean_text(notes);\n",
    "#pickle.dump(notes_filtered, open('/home/ubuntu/CliNER/data/saved/notes_filtered', \"wb\"), protocol=2);\n",
    "notes_filtered = pickle.load(open('/home/ubuntu/CliNER/data/saved/notes_filtered','rb'));\n",
    "#notes_filtered = notes_filtered[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A method to parse the output from CliNER into a vocabulary. We use this vocab to inform out TFIDF Transformer in order to reduce our vector size from millions down to 64k important words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notify_slack(\"Getting CliNER vocab\")\n",
    "\n",
    "def get_cliner_vocab():\n",
    "    text_list = []\n",
    "    type_list = []\n",
    "    for f in os.listdir('/home/ubuntu/CliNER/data/CLEANED/'):\n",
    "        with open('/home/ubuntu/CliNER/data/CLEANED/' + f, 'rb') as file:\n",
    "            for line in file:\n",
    "                matches_text = re.search('(?<=c=\\\").*?(?=\\\" )', line); # gets the highlighted text\n",
    "                matches_text_group0 = re.sub(\"[^a-zA-Z0-9]\", \" \", matches_text.group(0))\n",
    "                matches_type = re.search('(?<=t=\\\").*?(?=\\\")', line); # gets the designation\n",
    "                text_list.append(matches_text_group0)\n",
    "                type_list.append(matches_type.group(0))\n",
    "    \n",
    "    return text_list, type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_list, type_list = get_cliner_vocab()\n",
    "max_length = max([len(text.split()) for text in text_list])\n",
    "min_length = min([len(text.split()) for text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaned_data_vectorizer = CountVectorizer(ngram_range=(min_length, max_length), stop_words='english');\n",
    "\n",
    "#cleaned_vectorizer_updated = cleaned_data_vectorizer.fit(text_list);\n",
    "\n",
    "#cleaned_notes_counts = cleaned_vectorizer_updated.transform(text_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 4), stop_words='english', min_df=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    notify_slack(\"Starting TFIDF Fit\")\n",
    "    cleaned_tfidf_vectorizer_updated = cleaned_tfidf_vectorizer.fit(text_list);\n",
    "\n",
    "    cleaned_tfidf_counts = cleaned_tfidf_vectorizer_updated.transform(notes_filtered[\"TEXT\"]);\n",
    "    \n",
    "    notify_slack(\"Pickling sparse counts into current directory\")\n",
    "    pickle.dump(cleaned_tfidf_counts, open(\"sparse_tfidf_min_df=0.05\",'wb'))\n",
    "    \n",
    "    notify_slack(\"Fitting with Truncated SVD\")\n",
    "    svd = TruncatedSVD(n_components = 1000)\n",
    "    clean_tfidf_reduced = svd.fit_transform(cleaned_tfidf_counts)\n",
    "    \n",
    "    #notify_slack(\"Converting to array\")\n",
    "    #cleaned_tfidf_counts_array = cleaned_tfidf_counts.toarray()\n",
    "    \n",
    "    notify_slack(\"Pickling array into current directory\")\n",
    "    pickle.dump(clean_tfidf_reduced, open(\"tfidf_vectors\",'wb'))\n",
    "    \n",
    "    notify_slack(\"Pickling SVD into current directory\")\n",
    "    pickle.dump(svd, open('fit_svd_model', 'wb'))\n",
    "    \n",
    "    notify_slack(\"Successfully completed! :D \")\n",
    "    \n",
    "except:\n",
    "    notify_slack(\"Crashed during TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved for  potential reuse\n",
    "---\n",
    "vectorizer = CountVectorizer();\n",
    "\n",
    "vectorizer_updated = vectorizer.fit(notes_filtered['TEXT']);\n",
    "\n",
    "notes_counts = vectorizer_updated.transform(notes_filtered['TEXT']);\n",
    "\n",
    "### Get top words for the report\n",
    "#notes_order = np.argsort(notes_counts.todense(), axis=0);\n",
    "\n",
    "notes_sum = np.sum(notes_counts, axis=0);\n",
    "\n",
    "notes_sort = np.argsort(-1 * notes_sum[0]);\n",
    "\n",
    "name_counts = pd.DataFrame({'name' : vectorizer_updated.vocabulary_.keys(), 'idx' : vectorizer_updated.vocabulary_.values()})\n",
    "#name_counts = name_counts.set_index('idx');\n",
    "\n",
    "sort_list = list();\n",
    "sum_list = list();\n",
    "for i in range(notes_sort.shape[1]):\n",
    "    sort_list.append(notes_sort[0,i]);\n",
    "    sum_list.append(notes_sum[0,i]);\n",
    "    \n",
    "count_df = name_counts.sort_values(by='idx', ascending=True);\n",
    "sum_df = pd.DataFrame({'count' : sum_list, 'idx' : np.arange(0,len(sum_list))});\n",
    "\n",
    "count_sum = count_df.join(sum_df, on='idx', lsuffix='c', rsuffix='s').drop('idxs', axis=1);\n",
    "count_sum_top = count_sum.sort_values(by='count', ascending=False);\n",
    "\n",
    "shuffle(count_sum_top.loc[count_sum_top['count'] == 1])\n",
    "\n",
    "name_counts.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
