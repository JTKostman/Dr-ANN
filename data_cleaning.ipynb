{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Cleaning and Processing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import scipy as sp;\n",
    "import re;\n",
    "import pickle;\n",
    "import os;\n",
    "import subprocess\n",
    "\n",
    "import sklearn;\n",
    "from sklearn.utils import shuffle;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer;\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to help us keep track of the process\n",
    "def notify_slack(text):\n",
    "    subprocess.Popen('''curl -X POST --data-urlencode \"payload={'channel' : '#random', 'username': 'webhookbot', 'text':'''+ '\\'' + text + '\\'' + '''}\" https://hooks.slack.com/services/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''', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load data</h4>\n",
    "\n",
    "<p>This function will read in data to be vectorized for the LSTM model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notify_slack(\"Starting data load\")\n",
    "\n",
    "def load_data():\n",
    "    diagnosis = pd.read_csv('/home/ubuntu/workspace/data/DIAGNOSES_ICD.csv');\n",
    "    notes = pd.read_csv('/home/ubuntu/workspace/data/NOTEEVENTS.csv');\n",
    "    \n",
    "    return diagnosis, notes;\n",
    "\n",
    "diagnosis, notes = load_data();\n",
    "diagnosis = diagnosis.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Format IC9_Codes </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notify_slack(\"Startin label generation\")\n",
    "groups = notes.groupby('HADM_ID').apply(lambda row: list(set(row['TEXT'])));\n",
    "\n",
    "# Takes string and returns a formated icd9 code\n",
    "def format_icd9(icd9):\n",
    "    if icd9[0] == \"V\":\n",
    "        return icd9[0:3]\n",
    "    if icd9[0] == \"E\":\n",
    "        return icd9[0:4]\n",
    "    else: \n",
    "        return icd9[0:3]\n",
    "\n",
    "formatted_icd9_codes = diagnosis[\"ICD9_CODE\"].apply(format_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diagnosis_reduced_icd9 = diagnosis.join(formatted_icd9_codes, lsuffix=\"_l\", rsuffix=\"_r\")\n",
    "diagnosis_reduced_icd9 = diagnosis_reduced_icd9[[\"HADM_ID\", \"ICD9_CODE_r\"]]\n",
    "diagnosis_reduced_icd9.columns = [\"HADM_ID\", \"ICD9_CODE\"]\n",
    "\n",
    "diagnosis_group_reduced = diagnosis_reduced_icd9.groupby('HADM_ID').apply(lambda x: set(x['ICD9_CODE']));\n",
    "diagnosis_count_reduced = diagnosis_group_reduced.apply(lambda x: len(x))\n",
    "diagnosis_group_reduced.name = \"ICD9_set\"\n",
    "\n",
    "notes_icd9 = notes.set_index(\"HADM_ID\").join(diagnosis_group_reduced, how=\"inner\", lsuffix=\"_l\", rsuffix=\"_r\")\n",
    "notes_icd9 = notes_icd9[[\"TEXT\", \"ICD9_set\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bool_print = False\n",
    "if bool_print:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    print (\"Overall: \", len(diagnosis))\n",
    "    print (\"Unique codes:\", len(set(diagnosis[\"ICD9_CODE\"].values)))\n",
    "    print (\"Reduced unique codes\", len(set(formatted_icd9_codes)))\n",
    "\n",
    "    diagnosis_group = diagnosis.groupby('HADM_ID').apply(lambda x: list(x['ICD9_CODE']));\n",
    "    diagnosis_count = diagnosis_group.apply(lambda x: len(x))\n",
    "    \n",
    "    diagnosis_count.hist(bins=35)\n",
    "    plt.show()\n",
    "\n",
    "    diagnosis_count_reduced.hist(bins=35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indexing into dict to get feature array mapping\n",
    "# takes in a icd9 set object\n",
    "def feature_mapping(icd9_set):\n",
    "    vector = np.zeros((len(icd9_mapping)))\n",
    "    for icd9 in icd9_set:\n",
    "        vector[icd9_mapping[icd9]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "icd9_mapping = dict(zip( set(diagnosis_reduced_icd9[\"ICD9_CODE\"].values) , np.arange(0, len(set(formatted_icd9_codes))) ))\n",
    "notes_icd9[\"vector\"] = notes_icd9[\"ICD9_set\"].apply(feature_mapping)\n",
    "labels = notes_icd9[\"vector\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(lables, open(\"labels\", 'wb'))\n",
    "#labels = pickle.load(open(\"labels\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a custom stop words method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stop_words = str();\n",
    "    with open('/home/ubuntu/workspace/CS-6250-Project/nltk', 'r') as f:\n",
    "        for line in f:\n",
    "            stop_words = stop_words + '\\\\b' + line.strip() + '\\\\b' + '|';\n",
    "\n",
    "    stop_words = stop_words[0:-1];\n",
    "\n",
    "    return stop_words;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that takes in a textual clinical note and run preprocessing steps on it to remove dates, lower case all the words, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(notes_df):\n",
    "    stop_words = get_stopwords();\n",
    "    \n",
    "    # Need to remove single charcater items\n",
    "    # Need to remove the leading 0 from a digit (i.e. 07 = 7) -OR- replace with \"DIGIT\"\n",
    "    # replace dates with \"DATE\" or something \n",
    "    \n",
    "    #notes_filtered = notes_df['TEXT'].apply(lambda row: re.sub(\"[^a-zA-Z0-9]\", \" \", row.lower()));\n",
    "    notes_filtered = notes_df['TEXT'].apply(lambda row: re.sub(\"21[0-9]{2}.[0-1]?[0-9]{1}.[0-3]?[0-9]{1}.+[0-2]{1}[0-9]{1}:[0-5]{1}[0-9]{1}.+[\\bAM\\b|\\bPM\\b]\", \" \", row));\n",
    "    notes_filtered = notes_filtered.apply(lambda row: re.sub(\"[^a-zA-Z0-9\\.]\", \" \", row.lower()));\n",
    "    #notes_filtered = notes_filtered.apply(lambda row: re.sub(\"\\W\\d+\\.?\\d*\", \"DIGIT\", row));\n",
    "    #notes_filtered = notes_filtered.apply(lambda row: re.sub(\"\\s[a-zA-Z]\\s\", \" \", row));                                        \n",
    "\n",
    "    notes_nostops = notes_filtered.apply(lambda row: re.sub(stop_words, ' ', row));\n",
    "    \n",
    "    notes_final = notes_nostops.apply(lambda row: \" \".join(row.split()));\n",
    "    \n",
    "    notes_df = notes_df.drop('TEXT', axis=1);\n",
    "    notes_df = notes_df.assign(TEXT = notes_final.values)\n",
    "    \n",
    "    return notes_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notes_filtered = clean_text(notes_icd9);\n",
    "pickle.dump(notes_filtered, open('notes_filtered', \"wb\"), protocol=2);\n",
    "#notes_filtered = pickle.load(open('notes_filtered','rb'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A method to parse the output from CliNER into a vocabulary. We use this vocab to inform out TFIDF Transformer in order to reduce our vector size from millions down to 64k important words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notify_slack(\"Getting CliNER vocab\")\n",
    "\n",
    "def get_cliner_vocab():\n",
    "    text_list = []\n",
    "    type_list = []\n",
    "    for f in os.listdir('/home/ubuntu/CliNER/data/CLEANED/'):\n",
    "        with open('/home/ubuntu/CliNER/data/CLEANED/' + f, 'rb') as file:\n",
    "            for line in file:\n",
    "                matches_text = re.search('(?<=c=\\\").*?(?=\\\" )', line); # gets the highlighted text\n",
    "                matches_text_group0 = re.sub(\"[^a-zA-Z0-9]\", \" \", matches_text.group(0))\n",
    "                matches_type = re.search('(?<=t=\\\").*?(?=\\\")', line); # gets the designation\n",
    "                text_list.append(matches_text_group0)\n",
    "                type_list.append(matches_type.group(0))\n",
    "    \n",
    "    return text_list, type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_list, type_list = get_cliner_vocab()\n",
    "max_length = max([len(text.split()) for text in text_list])\n",
    "min_length = min([len(text.split()) for text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaned_data_vectorizer = CountVectorizer(ngram_range=(min_length, max_length), stop_words='english');\n",
    "\n",
    "#cleaned_vectorizer_updated = cleaned_data_vectorizer.fit(text_list);\n",
    "\n",
    "#cleaned_notes_counts = cleaned_vectorizer_updated.transform(text_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c403aa27df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned_tfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 4), stop_words='english', min_df=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    notify_slack(\"Starting TFIDF fit & transform\")\n",
    "    cleaned_tfidf_vectorizer_updated = cleaned_tfidf_vectorizer.fit(text_list);\n",
    "    cleaned_tfidf_counts = cleaned_tfidf_vectorizer_updated.transform(notes_filtered[\"TEXT\"]);\n",
    "    \n",
    "    notify_slack(\"Pickling sparse counts into current directory\")\n",
    "    pickle.dump(cleaned_tfidf_counts, open(\"cleaned_tfidf_counts\",'wb'))\n",
    "    \n",
    "    notify_slack(\"Fitting with Truncated SVD\")\n",
    "    svd = TruncatedSVD(n_components = 1000)\n",
    "    clean_tfidf_reduced = svd.fit_transform(cleaned_tfidf_counts)\n",
    "    \n",
    "    notify_slack(\"Pickling array into current directory\")\n",
    "    pickle.dump(clean_tfidf_reduced, open(\"clean_tfidf_reduced\",'wb'))\n",
    "    \n",
    "    notify_slack(\"Pickling SVD into current directory\")\n",
    "    pickle.dump(svd, open('fit_svd_model', 'wb'))\n",
    "    \n",
    "    notify_slack(\"Successfully completed! :D \")\n",
    "    \n",
    "except:\n",
    "    notify_slack(\"Crashed during TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved for  potential reuse\n",
    "---\n",
    "vectorizer = CountVectorizer();\n",
    "\n",
    "vectorizer_updated = vectorizer.fit(notes_filtered['TEXT']);\n",
    "\n",
    "notes_counts = vectorizer_updated.transform(notes_filtered['TEXT']);\n",
    "\n",
    "### Get top words for the report\n",
    "#notes_order = np.argsort(notes_counts.todense(), axis=0);\n",
    "\n",
    "notes_sum = np.sum(notes_counts, axis=0);\n",
    "\n",
    "notes_sort = np.argsort(-1 * notes_sum[0]);\n",
    "\n",
    "name_counts = pd.DataFrame({'name' : vectorizer_updated.vocabulary_.keys(), 'idx' : vectorizer_updated.vocabulary_.values()})\n",
    "#name_counts = name_counts.set_index('idx');\n",
    "\n",
    "sort_list = list();\n",
    "sum_list = list();\n",
    "for i in range(notes_sort.shape[1]):\n",
    "    sort_list.append(notes_sort[0,i]);\n",
    "    sum_list.append(notes_sum[0,i]);\n",
    "    \n",
    "count_df = name_counts.sort_values(by='idx', ascending=True);\n",
    "sum_df = pd.DataFrame({'count' : sum_list, 'idx' : np.arange(0,len(sum_list))});\n",
    "\n",
    "count_sum = count_df.join(sum_df, on='idx', lsuffix='c', rsuffix='s').drop('idxs', axis=1);\n",
    "count_sum_top = count_sum.sort_values(by='count', ascending=False);\n",
    "\n",
    "shuffle(count_sum_top.loc[count_sum_top['count'] == 1])\n",
    "\n",
    "name_counts.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
