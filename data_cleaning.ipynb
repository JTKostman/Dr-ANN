{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Cleaning and Processing </h1>\n",
    "\n",
    "In this notebook, we will load in the the Dataset obtained from MIMIC III, and process it for Feature Construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first start by loading in the required libraries. This includes Pandas, Numpy, and Scipy for their data structures, SKLearn for Vectorization, and some system libraries such as Pickle and Subprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import re;\n",
    "import pickle;\n",
    "import subprocess;\n",
    "\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import scipy as sp;\n",
    "\n",
    "import sklearn;\n",
    "from sklearn.utils import shuffle;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer;\n",
    "from sklearn.decomposition import TruncatedSVD;\n",
    "from sklearn.externals import joblib;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to monitor our progress, we use a Webhook on Slack which notifies our channel with messages when certain milestones are reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to send notifications on Slack via Post CURL.\n",
    "'''\n",
    "def notify_slack(text):\n",
    "    subprocess.Popen('''curl -X POST --data-urlencode \"payload={'channel' : '#random', 'username': 'webhookbot', 'text':'''+ '\\'' + text + '\\'' + '''}\" https://hooks.slack.com/services/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''', shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load data</h4>\n",
    "\n",
    "We load in the necessary data files next, and drop rows with NA values to have clean dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify_slack(\"Starting data load\")\n",
    "\n",
    "'''\n",
    "Loads in the Dataset from CSV files and places them in a Pandas Dataframe.\n",
    "'''\n",
    "def load_data():\n",
    "    diagnosis = pd.read_csv('/home/ubuntu/workspace/data/DIAGNOSES_ICD.csv');\n",
    "    notes = pd.read_csv('/home/ubuntu/workspace/data/NOTEEVENTS.csv');\n",
    "    \n",
    "    return diagnosis, notes;\n",
    "\n",
    "diagnosis, notes = load_data();\n",
    "\n",
    "'''Drop rows with NA values'''\n",
    "diagnosis = diagnosis.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Format IC9_Codes </h4>\n",
    "\n",
    "We will be using the Broader ICD9 Codes instead of the longer form to allocate more rows for each label. This pre-processing step removes all digits from ICD9 code after the decimal point. This means that the ICD9 codes `250.01` and `250.02` will map to the same label, `250`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify_slack(\"Starting label generation\")\n",
    "\n",
    "groups = notes.groupby('HADM_ID').apply(lambda row: list(set(row['TEXT'])));\n",
    "\n",
    "'''\n",
    "Takes a String ICD9 code and returns it's truncated value.\n",
    "'''\n",
    "def format_icd9(icd9):\n",
    "    if icd9[0] == \"V\":\n",
    "        return icd9[0:3]\n",
    "    if icd9[0] == \"E\":\n",
    "        return icd9[0:4]\n",
    "    else: \n",
    "        return icd9[0:3]\n",
    "\n",
    "#Truncated ICD9s are obtained for each row.\n",
    "formatted_icd9_codes = diagnosis[\"ICD9_CODE\"].apply(format_icd9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained the truncated ICD9 codes, we next join them with our Notes dataset on Hospital ID (HADM_ID). This is the primary key common between both tables. Because there is a many-to-many relationship between the set of Notes for a patient and the set of ICD9 codes, we first create a SET from the list of ICD9 codes associated with a patient's hospital visit, and perform an inner join for it with the Notes table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Attach the Formatted ICD9 codes as a column to the Diagnosis table.\n",
    "'''\n",
    "diagnosis_reduced_icd9 = diagnosis.join(formatted_icd9_codes, lsuffix=\"_l\", rsuffix=\"_r\")\n",
    "diagnosis_reduced_icd9 = diagnosis_reduced_icd9[[\"HADM_ID\", \"ICD9_CODE_r\"]]\n",
    "diagnosis_reduced_icd9.columns = [\"HADM_ID\", \"ICD9_CODE\"]\n",
    "\n",
    "'''\n",
    "Create a SET ov ICD9 code values for each Hospital visit in Diagnosis.\n",
    "'''\n",
    "diagnosis_group_reduced = diagnosis_reduced_icd9.groupby('HADM_ID').apply(lambda x: set(x['ICD9_CODE']));\n",
    "diagnosis_count_reduced = diagnosis_group_reduced.apply(lambda x: len(x))\n",
    "diagnosis_group_reduced.name = \"ICD9_set\"\n",
    "\n",
    "'''\n",
    "Attach the SET of ICD9 code values as a column to the Notes table.\n",
    "'''\n",
    "notes_icd9 = notes.set_index(\"HADM_ID\").join(diagnosis_group_reduced, how=\"inner\", lsuffix=\"_l\", rsuffix=\"_r\")\n",
    "notes_icd9 = notes_icd9[[\"TEXT\", \"ICD9_set\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to examine our Table of ICD9 codes and it's distribution. To do this, we use a Histogram on the counts of each patient's ICD9 codes.\n",
    "\n",
    "# TODO: MORE PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_print = False\n",
    "if bool_print:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    print (\"Overall: \", len(diagnosis))\n",
    "    print (\"Unique codes:\", len(set(diagnosis[\"ICD9_CODE\"].values)))\n",
    "    print (\"Reduced unique codes\", len(set(formatted_icd9_codes)))\n",
    "\n",
    "    diagnosis_group = diagnosis.groupby('HADM_ID').apply(lambda x: list(x['ICD9_CODE']));\n",
    "    diagnosis_count = diagnosis_group.apply(lambda x: len(x))\n",
    "    \n",
    "    diagnosis_count.hist(bins=35)\n",
    "    plt.show()\n",
    "\n",
    "    diagnosis_count_reduced.hist(bins=35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in obtaining our labels is to create K-Hot encoded vectors from them. Each vector for a Hospital admission will consist of a Length-N vector with K values set to 1. N is the total number of ICD9 codes in our mult-class classification, and K is the total number of ICD9 code associated with a person's hospital visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creates a K-Hot encoded vector from a list of ICD9 \\\n",
    "codes using a dictionary mapping.\n",
    "'''\n",
    "def feature_mapping(icd9_set):\n",
    "    vector = np.zeros((len(icd9_mapping)))\n",
    "    for icd9 in icd9_set:\n",
    "        vector[icd9_mapping[icd9]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the above function on each row of the Notes-Icd9 Joined table. We will use a dictionary to map each ICD9 code to a position on the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_mapping = dict(zip( set(diagnosis_reduced_icd9[\"ICD9_CODE\"].values) , np.arange(0, len(set(formatted_icd9_codes))) ))\n",
    "notes_icd9[\"vector\"] = notes_icd9[\"ICD9_set\"].apply(feature_mapping)\n",
    "labels = notes_icd9[\"vector\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving progress with Pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(labels, \"saved_values/labels\")\n",
    "#labels = joblib.load(\"saved_labels/labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Extraction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by cleaning each Clinical note by removing special punctuation, Common stop words, and dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using NLTK's Stopwords vocabulary. In order to parse them in as a regular expression, they are read in and added to a regex-encoded string that can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reads a list of Stopwords from a file and adds them to a Regex expression.\n",
    "'''\n",
    "def get_stopwords():\n",
    "    stop_words = str();\n",
    "    with open('/home/ubuntu/workspace/CS-6250-Project/nltk', 'r') as f:\n",
    "        for line in f:\n",
    "            stop_words = stop_words + '\\\\b' + line.strip() + '\\\\b' + '|';\n",
    "\n",
    "    stop_words = stop_words[0:-1];\n",
    "\n",
    "    return stop_words;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that takes in a textual clinical note and run preprocessing steps on it to remove dates, lower case all the words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Cleans clinical text by removing Stop words, special punctuation, upper-case characters, and dates.\n",
    "'''\n",
    "def clean_text(notes_df):\n",
    "\n",
    "    #Stop words are obtained as a regex-expression.\n",
    "    stop_words = get_stopwords();\n",
    "    \n",
    "    # Need to remove single charcater items\n",
    "    # Need to remove the leading 0 from a digit (i.e. 07 = 7) -OR- replace with \"DIGIT\"\n",
    "    # replace dates with \"DATE\" or something \n",
    "    \n",
    "    #notes_filtered = notes_df['TEXT'].apply(lambda row: re.sub(\"[^a-zA-Z0-9]\", \" \", row.lower()));\n",
    "    \n",
    "    #Dates are removed using a special regex expression.\n",
    "    notes_filtered = notes_df['TEXT'].apply(lambda row: re.sub(\"21[0-9]{2}.[0-1]?[0-9]{1}.[0-3]?[0-9]{1}.+[0-2]{1}[0-9]{1}:[0-5]{1}[0-9]{1}.+[\\bAM\\b|\\bPM\\b]\", \" \", row));\n",
    "    \n",
    "    #Special punctuation and upper case characters are removed.\n",
    "    notes_filtered = notes_filtered.apply(lambda row: re.sub(\"[^a-zA-Z0-9\\.]\", \" \", row.lower()));\n",
    "    #notes_filtered = notes_filtered.apply(lambda row: re.sub(\"\\W\\d+\\.?\\d*\", \"DIGIT\", row));\n",
    "    #notes_filtered = notes_filtered.apply(lambda row: re.sub(\"\\s[a-zA-Z]\\s\", \" \", row));                                        \n",
    "\n",
    "    #Stop words are removed.\n",
    "    notes_nostops = notes_filtered.apply(lambda row: re.sub(stop_words, ' ', row));\n",
    "    \n",
    "    #Non-standard whitespace characters are removed.\n",
    "    notes_final = notes_nostops.apply(lambda row: \" \".join(row.split()));\n",
    "    \n",
    "    notes_df = notes_df.drop('TEXT', axis=1);\n",
    "    notes_df = notes_df.assign(TEXT = notes_final.values)\n",
    "    \n",
    "    return notes_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Notes-ICD9 Joined dataframe is passed into the clean-text function and the results are saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_filtered = clean_text(notes_icd9);\n",
    "joblib.dump(notes_filtered, \"saved_vales/notes_filtered\")\n",
    "#notes_filtered = joblib.load(\"saved_values/notes_filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A method to parse the output from CliNER into a vocabulary. We use this vocab to inform out TFIDF Transformer in order to reduce our vector size from millions down to 64k important words. \n",
    "\n",
    "The next step in Featurization happened offline. We used an Open-Source project called [Cliner](http://cliner.org/), and modified it for our needs. Cliner is a Python framework for parsing Clinical text with a trained Conditional Random Fields model. We use a pre-trained MIMIC Silver model, and pass in our Clinical Text to obtain properly extracted and encoded Problems, Treatments and Tests. The results from the app are written to a file, which is read in the following function and appended to a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify_slack(\"Getting CliNER vocab\")\n",
    "\n",
    "'''\n",
    "Gets the vocabulary predicted from Cliner by parsing the XML Lines and \\\n",
    "extracting the necessary terms from them, such as the value and the type.\n",
    "'''\n",
    "def get_cliner_vocab():\n",
    "    text_list = []\n",
    "    type_list = []\n",
    "    for f in os.listdir('/home/ubuntu/CliNER/data/CLEANED/'):\n",
    "        with open('/home/ubuntu/CliNER/data/CLEANED/' + f, 'rb') as file:\n",
    "            for line in file:\n",
    "                matches_text = re.search('(?<=c=\\\").*?(?=\\\" )', line); # gets the highlighted text\n",
    "                matches_text_group0 = re.sub(\"[^a-zA-Z0-9]\", \" \", matches_text.group(0))\n",
    "                matches_type = re.search('(?<=t=\\\").*?(?=\\\")', line); # gets the designation\n",
    "                text_list.append(matches_text_group0)\n",
    "                type_list.append(matches_type.group(0))\n",
    "    \n",
    "    return text_list, type_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the above function to obtrain the Texts and Types list. We will be extracting N-Grams so we obtain the Min and Max length of the pharses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_list, type_list = get_cliner_vocab()\n",
    "max_length = max([len(text.split()) for text in text_list])\n",
    "min_length = min([len(text.split()) for text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 4), stop_words='english', min_df=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parsed features from the Cliner Vocabulary, we will use Sklearn's Vectorizer models to featurize the values. We will then apply Principal Component Analysis by using SVD on the features. This reduces our feature size to 1000 features. A Smaller feature size will improve our model because more relavant features will be kept, and it will also speed up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    notify_slack(\"Starting TFIDF fit & transform\")\n",
    "    cleaned_tfidf_vectorizer_fit = cleaned_tfidf_vectorizer.fit(text_list);\n",
    "    cleaned_tfidf_counts = cleaned_tfidf_vectorizer_fit.transform(notes_filtered[\"TEXT\"]);\n",
    "    \n",
    "    notify_slack(\"Fitting with Truncated SVD\")\n",
    "    svd = TruncatedSVD(n_components = 1000)\n",
    "    cleaned_tfidf_reduced = svd.fit_transform(cleaned_tfidf_counts)\n",
    "    \n",
    "    notify_slack(\"Pickling fit SVD into current directory\")\n",
    "    joblib.dump(svd, \"saved_values/fit_svd\")\n",
    "    \n",
    "    notify_slack(\"Pickling feature vectors into current directory\")\n",
    "    joblib.dump(cleaned_tfidf_reduced,\"saved_values/cleaned_tfidf_reduced\")\n",
    "        \n",
    "    notify_slack(\"Successfully completed! :D \")\n",
    "    \n",
    "except:\n",
    "    notify_slack(\"Crashed during TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved for  potential reuse\n",
    "---\n",
    "\n",
    "    vectorizer = CountVectorizer();\n",
    "\n",
    "    vectorizer_updated = vectorizer.fit(notes_filtered['TEXT']);\n",
    "\n",
    "    notes_counts = vectorizer_updated.transform(notes_filtered['TEXT']);\n",
    "\n",
    "---\n",
    "### Get top words for the report\n",
    "---\n",
    "    #notes_order = np.argsort(notes_counts.todense(), axis=0);\n",
    "\n",
    "    notes_sum = np.sum(notes_counts, axis=0);\n",
    "\n",
    "    notes_sort = np.argsort(-1 * notes_sum[0]);\n",
    "\n",
    "    name_counts = pd.DataFrame({'name' : vectorizer_updated.vocabulary_.keys(), 'idx' : vectorizer_updated.vocabulary_.values()})\n",
    "    #name_counts = name_counts.set_index('idx');\n",
    "\n",
    "    sort_list = list();\n",
    "    sum_list = list();\n",
    "    for i in range(notes_sort.shape[1]):\n",
    "        sort_list.append(notes_sort[0,i]);\n",
    "        sum_list.append(notes_sum[0,i]);\n",
    "\n",
    "    count_df = name_counts.sort_values(by='idx', ascending=True);\n",
    "    sum_df = pd.DataFrame({'count' : sum_list, 'idx' : np.arange(0,len(sum_list))});\n",
    "\n",
    "    count_sum = count_df.join(sum_df, on='idx', lsuffix='c', rsuffix='s').drop('idxs', axis=1);\n",
    "    count_sum_top = count_sum.sort_values(by='count', ascending=False);\n",
    "\n",
    "    shuffle(count_sum_top.loc[count_sum_top['count'] == 1])\n",
    "\n",
    "    name_counts.sort_values(by='count', ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #cleaned_data_vectorizer = CountVectorizer(ngram_range=(min_length, max_length), stop_words='english');\n",
    "\n",
    "    #cleaned_vectorizer_updated = cleaned_data_vectorizer.fit(text_list);\n",
    "\n",
    "    #cleaned_notes_counts = cleaned_vectorizer_updated.transform(text_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
